import sys, os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import queue, threading, time, re, subprocess
import numpy as np
import sounddevice as sd
from scipy.io.wavfile import write
from scipy.signal import butter, lfilter
from loguru import logger
from config import config
from stream_tts import tts_manager
from typing import Final
import wave

conversation_active: Final[threading.Event] = threading.Event()

# === ä¼˜åŒ–åçš„å‚æ•° ===
# é¦–å…ˆæ£€æµ‹è®¾å¤‡æ”¯æŒçš„é‡‡æ ·ç‡
try:
    default_samplerate = sd.query_devices(sd.default.device[0], 'input')['default_samplerate']
    if default_samplerate < 16000:
        SAMPLERATE = 44100  # å›é€€åˆ°å¸¸è§é‡‡æ ·ç‡
    else:
        SAMPLERATE = 16000  # ä¼˜å…ˆä½¿ç”¨16000
except:
    SAMPLERATE = 44100  # é»˜è®¤å›é€€å€¼

BLOCKSIZE = 1024
SILENCE_THRESHOLD = 20.0
SILENCE_DURATION = 0.8
MAX_DURATION = 8
FIXED_WAV_PATH = "/dev/shm/voice_input.wav"
PRE_SPEECH_BUFFER_SIZE = 24

# æ‰“å°å®é™…ä½¿ç”¨çš„é‡‡æ ·ç‡
logger.info(f"ğŸ”Š Using sample rate: {SAMPLERATE}Hz")

# === éŸ³é¢‘æ»¤æ³¢å‡½æ•° ===
def butter_highpass(cutoff, fs, order=5):
    nyq = 0.5 * fs
    normal_cutoff = cutoff / nyq
    b, a = butter(order, normal_cutoff, btype='high', analog=False)
    return b, a

def highpass_filter(data, cutoff=100, fs=SAMPLERATE, order=5):
    b, a = butter_highpass(cutoff, fs, order=order)
    y = lfilter(b, a, data)
    return y

# === æ¸…ç†æ–‡æœ¬ ===
def _clean(text: str) -> str:
    return re.sub(r'[^\w\s]', '', text).lower().strip()

# === æ ‡å‡†å†™å…¥ wav æ–‡ä»¶ ===
def save_wav_standard(wav_path, audio_int16, samplerate):
    with wave.open(wav_path, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)  # 16-bit PCM
        wf.setframerate(samplerate)
        wf.writeframes(audio_int16.tobytes())

# === å½•éŸ³ç›´åˆ°é™éŸ³ç»“æŸ ===
def record_until_silence(threshold=SILENCE_THRESHOLD,
                         silence_duration=SILENCE_DURATION,
                         max_duration=MAX_DURATION) -> str:
    q_local = queue.Queue()
    silence_blocks = int(silence_duration * SAMPLERATE / BLOCKSIZE)
    max_blocks = int(max_duration * SAMPLERATE / BLOCKSIZE)

    pre_speech_buffer = []
    audio_blocks = []
    silence_counter = 0
    is_recording = False

    def cb(indata, frames, time_info, status):
        if status:
            logger.warning(f"âš ï¸ Audio status: {status}")
        q_local.put(indata.copy())

    logger.info("ğŸ™ï¸ Waiting for speech to start...")

    try:
        with sd.InputStream(samplerate=SAMPLERATE, channels=1,
                          blocksize=BLOCKSIZE, callback=cb, dtype='float32'):
            while True:
                try:
                    block = q_local.get(timeout=1)
                except queue.Empty:
                    continue

                volume = np.abs(block).mean() * 1000
                logger.debug(f"ğŸ“Š Vol: {volume:.1f}")

                pre_speech_buffer.append(block)
                if len(pre_speech_buffer) > PRE_SPEECH_BUFFER_SIZE:
                    pre_speech_buffer.pop(0)

                if not is_recording:
                    if volume > threshold:
                        logger.info("ğŸ”´ Voice detected. Start recording...")
                        is_recording = True
                        audio_blocks.extend(pre_speech_buffer)
                        audio_blocks.append(block)
                    continue

                audio_blocks.append(block)

                if volume < threshold:
                    silence_counter += 1
                    if silence_counter >= silence_blocks:
                        logger.info("ğŸ”‡ Silence detected. Stopping recording.")
                        break
                else:
                    silence_counter = 0

                if len(audio_blocks) >= max_blocks:
                    logger.info("â° Max recording length reached. Forcing stop.")
                    break

    except Exception as e:
        logger.error(f"âŒ Audio input error: {str(e)}")
        raise

    pcm_f32 = np.concatenate(audio_blocks).flatten()
    pcm_f32 = highpass_filter(pcm_f32)
    pcm_i16 = (pcm_f32 * 32767).clip(-32768, 32767).astype(np.int16)

    save_wav_standard(FIXED_WAV_PATH, pcm_i16, SAMPLERATE)
    logger.success(f"ğŸ’¾ Saved recording to {FIXED_WAV_PATH}")
    return FIXED_WAV_PATH

# === è°ƒç”¨ whisper-cli è½¬å½• ===
def transcribe_audio(wav_path: str, delay: float = 0.0) -> str:
    model_path = os.path.expanduser("~/whisper.cpp/models/ggml-tiny.en.bin")
    cli_path = os.path.expanduser("~/whisper.cpp/build/bin/whisper-cli")
    
    # å¦‚æœé‡‡æ ·ç‡ä¸æ˜¯16000ï¼Œéœ€è¦æ·»åŠ --rateå‚æ•°
    if SAMPLERATE != 16000:
        cmd = [cli_path, "-m", model_path, "-f", wav_path, "-t", "2", "--step", "500", "--length", "500", "--rate", str(SAMPLERATE)]
    else:
        cmd = [cli_path, "-m", model_path, "-f", wav_path, "-t", "2", "--step", "500", "--length", "500"]

    result = subprocess.run(cmd, capture_output=True, text=True)
    output = result.stdout.strip()

    lines = output.splitlines()
    text_lines = [
        line.split("]", 1)[-1].strip(" -\t") for line in lines
        if "-->" in line and "]" in line
    ]
    raw_text = " ".join(text_lines).strip()
    clean_text = _clean(raw_text)

    logger.success(f"ğŸ“ Transcribed Text: {clean_text or '<EMPTY>'}")
    if delay:
        time.sleep(delay)
    return clean_text

# === è¯†åˆ«å‡½æ•° ===
def recognize(delay: float = 0.0) -> str:
    wav_path = record_until_silence()
    return transcribe_audio(wav_path, delay)

# === åå°çƒ­è¯è¯†åˆ«çº¿ç¨‹ ===
def Whisper_run(callback_func):
    def loop():
        print("ğŸŸ¢ Whisper hotword loop started")
        while True:
            if conversation_active.is_set():
                time.sleep(0.2)
                continue

            try:
                raw_text = recognize(delay=3)
                clean_text = _clean(raw_text)
                if not clean_text:
                    continue

                if "open robot system" in clean_text:
                    config.set(chat_or_instruct=False)
                    logger.info("ğŸ® Switched to CONTROL mode.")
                    tts_manager.say("Okay, I'm now in control mode.")
                    conversation_active.set()
                    callback_func()

                elif "hi assistant" in clean_text:
                    config.set(chat_or_instruct=True)
                    logger.info("ğŸ’¬ Switched to CHAT mode.")
                    tts_manager.say("Sure, I'm now in chat mode.")
                    conversation_active.set()
                    callback_func()

                elif clean_text in {"ok bye", "okay bye", "ok byebye", "okay byebye"}:
                    tts_manager.say("Goodbye!")
                    time.sleep(1)
                    os._exit(0)
                    
            except Exception as e:
                logger.error(f"âŒ Recognition error: {str(e)}")
                time.sleep(1)  # é˜²æ­¢å¿«é€Ÿé‡è¯•å¯¼è‡´CPUå ç”¨è¿‡é«˜

    threading.Thread(target=loop, daemon=True).start()

# === æµ‹è¯•å…¥å£ ===
if __name__ == "__main__":
    # æ‰“å°éŸ³é¢‘è®¾å¤‡ä¿¡æ¯
    print("ğŸ”Š Available audio devices:")
    print(sd.query_devices())
    print(f"ğŸ”Š Default input device: {sd.default.device[0]}")
    
    logger.info("ğŸ¤ Start single recognition test...")
    result = recognize()
    print("ğŸ—£ï¸ You said:", result or "<nothing>")