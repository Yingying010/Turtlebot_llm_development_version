from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re
import ast
import json
import time
from textwrap import dedent
from stream_tts import tts_manager
import threading
from loguru import logger
from config import config

 
# ========== åŠ è½½æ¨¡å‹å’Œ Tokenizer ==========
print("â³ Loading tokenizer and model...")
load_start = time.time()
model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
load_end = time.time()
print(f"âœ… Model loaded in {load_end - load_start:.2f} seconds")


# ====== Prompt æ„é€ å‡½æ•° ======
def build_prompt(user_text: str) -> str:
    return f"""You are Lumi, a thoughtful and emotionally aware AI assistant who enjoys chatting casually with a human.
        You donâ€™t have access to real-time data, but can simulate reasoning and give friendly replies.
        Avoid repeating phrases, listing too much, or writing multiple turns of conversation. Just give a natural 2â€“4 sentence reply.

        Human says: "{user_text}"
        Lumi replies:"""

# ========== ç”Ÿæˆå‡½æ•° ==========
def generate_response(user_input, max_new_tokens=256):
    prompt = build_prompt(user_input)
    print("\nğŸŒ€ Generating response...")
    gen_start = time.time()
    inputs = tokenizer(prompt, return_tensors="pt")  # âœ… ç”¨ promptï¼Œè€Œä¸æ˜¯ user_input

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    gen_end = time.time()
    print(f"âœ… Generation completed in {gen_end - gen_start:.2f} seconds")

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded[len(prompt):].strip()


def run(user_input):
    logger.info(f"ğŸ’¡ Current Mode: {'Chat' if config.get('chat_or_instruct') else 'Control'}")
    logger.info(f"ğŸ§  LLM Input: {user_input}")
    output = generate_response(user_input)
    # print("\nğŸ§  åŸå§‹æ¨¡å‹è¾“å‡º:\n", output)
    if "Human says:" in output:
        response = output.split("Human says:")[0].strip().strip('"')
    else:
        response = output.strip()
    print("\nğŸ‰ Response:\n", response)

    return response


# ========== æ‰§è¡Œå‘½ä»¤ ==========
def execute_commands(result):
    tts_manager.say("Executing command")
    return ""

 
# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    user_input = "What is 1 plus 1?"
    response = run(user_input)


