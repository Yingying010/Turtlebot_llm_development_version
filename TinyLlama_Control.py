from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
import torch
import time
import re
import json
from textwrap import dedent
from deepdiff import DeepDiff
from datetime import datetime

import torch, re, json, time
from textwrap import dedent
from stream_tts import tts_manager
from loguru import logger
from config import config
import control_turtlebot

#============== å·¥å…·å‡½æ•° =================
def extract_json(text: str):
  # æ‰¾åˆ° "Assistant:" çš„ä½ç½®
    assistant_idx = text.lower().find("assistant:")
    if assistant_idx == -1:
        print("âŒ æ²¡æ‰¾åˆ° 'Assistant:' æ ‡ç­¾")
        return None

    # ä» assistant éƒ¨åˆ†å¼€å§‹
    content = text[assistant_idx:]

    # å¯»æ‰¾ç¬¬ä¸€ä¸ªåˆæ³• JSON å­—ç¬¦ä¸²ï¼ˆä½¿ç”¨å¤§æ‹¬å·é…å¯¹ï¼‰
    start = content.find('{')
    while start != -1:
        stack = 0
        for i in range(start, len(content)):
            if content[i] == '{':
                stack += 1
            elif content[i] == '}':
                stack -= 1
                if stack == 0:
                    json_str = content[start:i+1]
                    try:
                        return json.loads(json_str)
                    except json.JSONDecodeError as e:
                        break  # å½“å‰æ‹¬å·å†…å®¹éæ³•ï¼Œæ¢ä¸‹ä¸€ä¸ª {
        start = content.find('{', start + 1)

    print("âŒ æ²¡æ‰¾åˆ°åˆæ³• JSON å—")
    return None


#============== åŠ è½½æ¨¡å‹ =================
local_path = Path("/models/Qwen3_base_instructions")
print("â³ Loading tokenizer and model...")
start = time.time()
print("âœ… [1] å¼€å§‹åŠ è½½ tokenizer")
tokenizer = AutoTokenizer.from_pretrained(str(local_path), trust_remote_code=True, local_files_only=True)
print("âœ… [2] tokenizer åŠ è½½å®Œæˆ")

print("âœ… [3] å¼€å§‹åŠ è½½æ¨¡å‹ç»“æ„")
model = AutoModelForCausalLM.from_pretrained(str(local_path), trust_remote_code=True, local_files_only=True)
print("âœ… [4] æ¨¡å‹ç»“æ„åŠ è½½å®Œæˆ")

print("âœ… [5] å¼€å§‹åŠ è½½åˆ°è®¾å¤‡")
device = "cpu"
model = model.to(device).eval()
print(f"ğŸ‰ [6] æ‰€æœ‰åŠ è½½å®Œæˆ Model loaded in {time.time() - start:.2f} seconds")


# ===== Prompt æ¨¡æ¿ =====
def inference(user_input):
    system_prompt = dedent('''
        You are a robot command parser. Given a natural language instruction, output ONLY a valid JSON object that maps each robot to a list of its actions.

        Each action must include:
        - "action": e.g. "move", "turn", "navigate", etc.
        - "direction": e.g. "left", "right", "forward", or null
        - "value": a number (e.g. 2.5) or null
        - "unit": e.g. "seconds", "meters", "degrees", or null
        - "target": either an object like {"x": ..., "y": ...}, or a name string like "user", or null

        Rules:
        - Output must be a valid JSON object.
        - Top-level keys must be robot identifiers in the form of <type><number>.
        - Each robot maps to a list of actions (1 or more).
        â— Output MUST start with '{' and end with '}'.
        â— Output MUST be a valid JSON object. No other text is allowed.
        âŒ Do NOT include plan, explanation, reasoning, thought, or any prefix like "Assistant:", "Plan:", "Structure:", "Here is".

    ''').strip()

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)


    #============== æ¨ç†æµç¨‹ =================

    print("\nğŸŒ€ Generating response...")
    gen_start = time.time()

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=256)
    raw_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print("===================Raw Result====================\n",raw_response)

    result = extract_json(raw_response)
    print("===================JSON Result====================\n",result)

    gen_end = time.time()
    print(f"âœ… Generation completed in {gen_end - gen_start:.2f} seconds")

    return raw_response, result


# ===== ä¸»æ‰§è¡Œå‡½æ•° =====
def run(user_input: str):
    logger.info(f"ğŸ’¡ Mode: {'Chat' if config.get('chat_or_instruct') else 'Control'}")
    logger.info(f"ğŸ§  LLM Input: {user_input}")
    
    commands = inference(user_input)
    if commands:
        logger.info("\nâœ… Parsed JSON:\n", commands)
        return commands
    else:
        print("\nâŒ Failed to extract valid JSON.")
        logger.warning("âš ï¸ No command was executed due to invalid JSON.")
        return None 

# ===== æµ‹è¯•å…¥å£ =====
if __name__ == "__main__":
    user_input = "let robot 1 turn right 45 degrees"
    response = run(user_input)
    if response:
        control_turtlebot.run(response)
        logger.info("âœ… Command(s) executed successfully.")
        tts_manager.say("Command executed.")
        tts_manager.wait_until_done()
    else:
        logger.warning("âš ï¸ No commands received from LLM.")
        tts_manager.say("Sorry, I couldn't understand the instruction.")
        tts_manager.wait_until_done()

