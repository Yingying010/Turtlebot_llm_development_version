import json
import re
import time
from loguru import logger
from const_config import param_types
from config import config
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from control_turtlebot import move_robot
from control_speaker import set_speaker_volume
import threading
from textwrap import dedent
from stream_tts import tts_manager

stop_generation_flag = threading.Event()  # ä¸­æ–­æ ‡å¿—

# ========== åŠ è½½æ¨¡å‹å’Œ Tokenizer ==========
model_path = "lora-tinyllama-turtlebot"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path).to(device)
 
# ========== æ„é€  Prompt (control version)==========
SYSTEM_PROMPT = dedent("""
    You are **TurtleBotCommandParser**, an expert JSON generator for multi-robot control.
 
    â–  Output **only** a JSON object that conforms to <SCHEMA>.
    â–  Before generating JSON, map any alias in <ALIASES> to its canonical robot_id.
    â–  Do NOT output any text other than the JSON.
 
    <ALIASES>
      "robot1"  ->  "turtlebot1"
      "robot2"  ->  "turtlebot2"
      "turtlebot one" -> "turtlebot1"
      "turtlebot two" -> "turtlebot2"
      // add more if needed
    </ALIASES>
    â€¢ Map aliases BOTH when used as robot IDs **and** when they appear as "target" values.
 
    <SCHEMA>
    {
      "$root": {
        "<robot_id>": [
          {
            "action"   : "move | turn | navigate_to",
            "direction": "forward | backward | left | right",   // optional for navigate_to
            "value"    : <number>,                              // distance (m), duration (s) or angle (deg)
            "unit"     : "meters | seconds | degrees",
            "target"   : "self" | {"x": <number>, "y": <number>} // navigate target or self
          }
        ]
      }
    }
    </SCHEMA>
 
    Think step-by-step *silently*, then output the JSON only.
""").strip()
 
EXAMPLES = [
    {
        "instruction": "let turtlebot1 forward 2 meters and turn left 45 degrees, "
                       "and let turtlebot2 backward 10 meters and turn right 10 degrees.",
        "response": {
            "turtlebot1": [
                {"action": "move", "direction": "forward", "value": 2,  "unit": "meters"},
                {"action": "turn", "direction": "left", "target": "self", "value": 45, "unit": "degrees", }
            ],
            "turtlebot2": [
                {"action": "move", "direction": "backward","value": 10, "unit": "meters"},
                {"action": "turn", "direction": "right", "target": "self", "value": 10, "unit": "degrees", }
            ]
        }
    },
    {
        "instruction": "let turtlebot1 forward 2 seconds and go to position (2.5, 3.0), "
                       "and let turtlebot2 turn right 45 degrees and go to position (100, 350)",
        "response": {
            "turtlebot1": [
                {"action": "move", "direction": "forward", "value": 2,  "unit": "seconds"},
                {"action": "navigate_to", "target": {
                    "x": 2.5,
                    "y": 3.0
                } }
            ],
            "turtlebot2": [
                {"action": "turn", "direction": "right", "target": "self", "value": 45, "unit": "degrees", },
                {"action": "navigate_to", "target": {
                    "x": 100,
                    "y": 350
                } }
            ]
        }
    },
    {
        "instruction": "let robot1 approach me",
        "response": {
            "turtlebot1": [
                {"action": "navigate_to","target": "user"}
            ],
        }
    },
    {
        "instruction": "robot1, go find robot2",
        "response": {
            "turtlebot1": [
            {"action": "navigate_to", "target": "turtlebot2"}
            ]
        }
    }
    # ğŸ‘‰ ç»§ç»­åœ¨è¿™é‡Œæ·»åŠ æ›´å¤š few-shot æ ·ä¾‹ï¼ˆä¿æŒç›¸åŒæ ¼å¼ï¼‰...
]
 
def build_prompt(instruction: str) -> str:
    """
    æ„é€ é«˜è´¨é‡ promptï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆç¬¦åˆ schema çš„ JSONã€‚
    """
    # Few-shot ç¤ºä¾‹å—
    few_shot_blocks = []
    for ex in EXAMPLES:
        few_shot_blocks.append(
            dedent(f"""
            <EXAMPLE>
                        ### Instruction:
                        {ex['instruction']}
                        ### Response:
                        {json.dumps(ex['response'], indent=2, ensure_ascii=False)}
            </EXAMPLE>
                        """).strip()
        )
    few_shot_section = "\n\n".join(few_shot_blocks)

    # ç”¨æˆ·çœŸå®æŒ‡ä»¤å—
    user_block = dedent(f"""
    <TASK>
        ### Instruction:
        {instruction}
        ### Response:
    """).strip()

    # æœ€ç»ˆ prompt
    prompt = f"{SYSTEM_PROMPT}\n\n{few_shot_section}\n\n{user_block}"
    return prompt


def get_system_prompt(user_input):
    is_chat = config.get("chat_or_instruct")
    if is_chat: 
        prompt = (
            "You are a warm and empathetic AI companion. Respond like a friend.\n\n"
            f"User: {user_input}\nAssistant:"
        )
    else:
        prompt = build_prompt(user_input)
    return prompt


# ========== ç”Ÿæˆresponse ==========
def generate_response(prompt: str, result_container: list[str]):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    tts_manager.say("Generating response")
    logger.info("â³ Generating response...")
    start_time = time.time()
    try:
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )
        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        decoded = decoded[len(prompt):].lstrip()
        duration = time.time() - start_time
        logger.info(f"âœ… Response generated in {duration:.2f} seconds.")
        result_container.append(decoded)
    except Exception as e:
        logger.error(f"âŒ Generation failed: {e}")

# ========== æå– JSON (control version)==========
def extract_json(text: str):
    fence_match = re.search(r"```(?:json)?\s*([\s\S]+?)\s*```", text, re.IGNORECASE)
    if fence_match:
        candidate = fence_match.group(1).strip()
        try:
            return json.loads(candidate)
        except Exception as e:
            print("âš ï¸ fenced JSON è§£æå¤±è´¥:", e)
    start = text.find('{')
    while start != -1:
        stack = 0
        for idx in range(start, len(text)):
            if text[idx] == '{':
                stack += 1
            elif text[idx] == '}':
                stack -= 1
                if stack == 0:          # æ‰¾åˆ°å¹³è¡¡
                    candidate = text[start:idx+1]
                    try:
                        return json.loads(candidate)
                    except Exception:
                        # è§£æå¤±è´¥ï¼Œç»§ç»­åœ¨åé¢æ‰¾
                        break
        start = text.find('{', start + 1)
    return None
 

# ========== è®°å½•æ—¥å¿—==========
def log_chat_to_file(user_input, response_text, filepath="log.txt"):
    """å°†èŠå¤©è¾“å…¥è¾“å‡ºè®°å½•åˆ° log.txt"""
    with open(filepath, "a") as f:
        f.write(f"\nğŸ§‘ User: {user_input}\nğŸ¤– Assistant: {response_text}\n{'='*40}\n")

# ========== ä¸»è¿è¡Œ ==========
def send(user_input):
    logger.info(f"ğŸ’¡ Current Mode: {'Chat' if config.get('chat_or_instruct') else 'Control'}")
    logger.info(f"ğŸ§  LLM Input: {user_input}")
    prompt = get_system_prompt(user_input)

    # æ¸…é™¤ä¸­æ–­æ ‡å¿—
    stop_generation_flag.clear()

    result_container: list[str] = []

    # å¯åŠ¨å­çº¿ç¨‹ç”Ÿæˆ
    gen_thread = threading.Thread(target=generate_response, args=(prompt, result_container))
    gen_thread.start()

    # æ£€æŸ¥æ˜¯å¦ä¸­æ–­
    while gen_thread.is_alive():
        if stop_generation_flag.is_set():
            logger.warning("ğŸ›‘ Generation interrupted by user.")
            return ""  # æˆ–è€… return None
        time.sleep(0.1)

    if not result_container:
        logger.warning("âŒ No response generated.")
        return ""

    output_text = result_container[0]
    logger.info(f"ğŸ§  LLM Output:\n{output_text}")

    log_chat_to_file(user_input, output_text)

    if config.get("chat_or_instruct"):
        # æå–ç¬¬ä¸€ä¸ª Assistant å›å¤
        match = re.findall(r"Assistant:\s*(.+)", output_text)
        first_response = match[0].strip() if match else output_text.strip()
        logger.info(f"ğŸ—£ï¸ Final chat reply: {first_response}")
        return first_response
    else:
        command = extract_json(output_text)
        if command:
            logger.info("\nJSON Command: \n", command)
            return command
        else:
            print("\nUnable to extract JSON, please check the output formatã€‚")

# ========== æ‰§è¡Œå‘½ä»¤ ==========
def execute_commands(result):
    tts_manager.say("Executing command")
    return ""
    